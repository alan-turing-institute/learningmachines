# Learning Machines Planning - Q1 2021

## Toy Model 1 (January, early February)

To demonstrate:

- A model trained on the SEER dataset (breast cancer cause of death prediction, retrained over time on patients with different years of diagnosis.

- Report the performance of each version of the model on different subsets of patients.

- Prototype front-end user-interface (UI)/demonstration tool to present the results.

- Infrastructure to:
  - Run and store the results of the models.
  - APIs (interfaces) for the front-end (UI) to get model results from the back-end (infrastructure).


## SEER (February, March)

- Explore additional SEER fields that may be relevant for our purposes (demographics, treatment etc.)

- Identify new questions/models likely to show significant differences over time, between different patient sub-groups, or similar.

- Literature review of other work on SEER/cancer datasets with a focus on:
  - Pre-existing predictive models
  - Large shifts/trends in cancer treatment & diagnosis (over time, between locations, in different demographics, etc.)

- Attempt to recreate the models developed in ["Reproducible Survival Prediction with SEER Cancer Data", S. Hegselmann et al., 2018](https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/5b7372be03ce645e7ad36d8a/1534292671844/11.pdf)
  - Integrate with ModMon/our infrastructure


## Report/Blog post (February, March)

- Collate thoughts on the components needed for a learning machine/long-term deployment of a ML model into a report.
  
- Gather case studies/examples of projects where each component is important.

- Write/finalise a blog post.


## MoJ (February, March)

- VIPER
  - Finalise statements of work and data access agreements.

    - Review prior work on VIPER (to the extent possible pre-agreements being in place).
    - MoJ to send documentation.

    - Identify tasks and where we can add value (will also be part of statements of work, but in more detail here for our own purposes), such as:
    - Reproduce current VIPER score and understand its use and suitability.
    - Explore the performance of VIPER across different sub-groups.
    - Identify limitations of the current approach (both from a modelling and data perspective).

    - Ensure infrastructure and tooling is in place to make prompt progress as soon as the project begins.

- RECALL
  - Continue to develop statements of work and explore where we might be able to contribute.

## Dataset(s) (March, April)

- Formally define dataset criteria for different aspects of the project (see themes below).
  - E.g. what is needed for drift detection, interpretibility, bias etc.

- Search for alternative/complementary datasets to SEER (_especially if it seems SEER may not fit all our criteria_):
  - Ask wider Turing community
  - Consider examples from outside healthcare & justice
  - Synthetic data


## Toy Model 2 (April-)

Desired features (to be identifed by SEER/dataset exploration above):

- Changes in model performance over time (e.g. model performs worse on new test data compared to training data), or across geography/demographics.

- Training and evaluating models in real-time rather than using pre-populated results.

- Descriptive stats (beyond basics)

- Drift detection

- Interpretibility

- Bias/sub-group analysis

- Improved dataset/code versioning, and reproducibility

## Data Science Research Themes (April-)

Agree on one or two core themes to focus on from the following:

- Drift, change-point and anomaly detection
  - Applied to both descriptive stats and model performance metrics.
  - Links with other REG projects, e.g. AIDA Lloyds for change-point and Gates for anomaly detection.

- Counterfactuals & treatment plans
  - How to evaluate the impact of a deployed model, e.g. the effectiveness of a treatment without knowing how other treatments would have performed for a patient.

- Interpretibility
  - Explaining model choices (particularly to a non-data scientist)

- Bias & Fairness
  - How to measure equality of model performance across different sub-groups.

- Model selection, (re-)training & resilience
  - When to retrain (similar to drift detection)?
  - How to retrain (e.g. should new data be weighted more heavily)?
  - Models/frameworks that are resilient to data changes and can be used for many different problems.

After selecting the theme(s):
- Review pre-existing work in the area and create a summary document (also see blog post/report)
- Install/use pre-existing examples
- Apply to LM


## Infrastructure (April-)

- Reproducibility

- Dataset, model and code versioning

- Deployed ML in the cloud
  - Explore (and attempt to deploy) pre-existing open source solutions for:
    - Getting new predictions from a model on streaming (live updating data)
    - Automated reporting of stats and metrics
    - Automated monitoring and triggering of retraining (and swapping live model to latest version after retraining)
  - Ask wider Turing community (e.g. Clean Air?)
  - Aim to create a template/barebones example that could be used by other projects?

- ModMon
  - Aim to develop into a publishable tool providing most of the above. Requires adding/improving:
    - Git integration + versioned storage
    - More flexible commands, database structure etc.
    - Tests, examples, documentation
    - (Stretch goal: Deployed real-time data updating and predictions)

- Generation of descriptive stats reports.

